# CS224n-2019

æœ¬`Repo`ä¸»è¦åŒ…æ‹¬è¯¾ç¨‹çš„ä½œä¸šä¸æ–‡æ¡£(Lecture, Note, Additional Readings, Suggested Readings)

è¯¾ç¨‹ç¬”è®°å‚è§æˆ‘çš„[åšå®¢](https://looperxx.github.io/CS224n-2019-01-Introduction%20and%20Word%20Vectors/)ï¼Œå¹¶åœ¨åšå®¢çš„[Repo](<https://github.com/LooperXX/LooperXX.github.io>)ä¸­æä¾›ç¬”è®°æºæ–‡ä»¶çš„ä¸‹è½½

æœ¬ç¬”è®°ä¸ä»…ä½¿ç”¨äº†Markdownçš„é€šç”¨è¯­æ³•ï¼Œè¿˜ä½¿ç”¨äº† mkdocs-material çš„ä¸€äº›è¯­æ³•ä»¥æå‡è§†è§‰æ•ˆæœï¼Œæ¨èé€šè¿‡æˆ‘çš„[åšå®¢](<https://looperxx.github.io/CS224n-2019-Assignment/>)è¿›è¡Œè®¿é—®ã€‚

## Assignment 01

-   é€æ­¥å®Œæˆå…±ç°çŸ©é˜µçš„æ­å»ºï¼Œå¹¶è°ƒç”¨ `sklearn.decomposition` ä¸­çš„ `TruncatedSVD` å®Œæˆä¼ ç»Ÿçš„åŸºäºSVDçš„é™ç»´ç®—æ³•
-   å¯è§†åŒ–å±•ç¤ºï¼Œè§‚å¯Ÿå¹¶åˆ†æå…¶åœ¨äºŒç»´ç©ºé—´ä¸‹çš„èšé›†æƒ…å†µã€‚
-   è½½å…¥Word2Vecï¼Œå°†å…¶ä¸SVDå¾—åˆ°çš„å•è¯åˆ†å¸ƒæƒ…å†µè¿›è¡Œå¯¹æ¯”ï¼Œåˆ†æä¸¤è€…è¯å‘é‡çš„ä¸åŒä¹‹å¤„ã€‚
-   å­¦ä¹ ä½¿ç”¨`gensim`ï¼Œä½¿ç”¨`Cosine Similarity` åˆ†æå•è¯çš„ç›¸ä¼¼åº¦ï¼Œå¯¹æ¯”å•è¯å’Œå…¶åŒä¹‰è¯ä¸åä¹‰è¯çš„`Cosine Distance` ï¼Œå¹¶å°è¯•æ‰¾åˆ°æ­£ç¡®çš„ä¸é”™è¯¯çš„ç±»æ¯”æ ·ä¾‹
-   æ¢å¯»Word2Vecå‘é‡ä¸­å­˜åœ¨çš„ `Independent Bias` é—®é¢˜

## Assignment 02

### 1  Written: Understanding word2vec

$$
P(O=o | C=c)=\frac{\exp \left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}{\sum_{w \in \mathrm{Vocab}} \exp \left(\boldsymbol{u}_{w}^{\top} \boldsymbol{v}_{c}\right)}
$$

$$
J_{\text { naive-softmax }}\left(v_{c}, o, U\right)=-\log P(O=o | C=c)
$$

çœŸå®(ç¦»æ•£)æ¦‚ç‡åˆ†å¸ƒ $p$ ä¸å¦ä¸€ä¸ªåˆ†å¸ƒ $q$ çš„äº¤å‰ç†µæŸå¤±ä¸º $-\sum_i p_{i} \log \left(q_{i}\right)$

!!! question "Question a"

```
Show that the naive-softmax loss given in Equation (2) is the same as the cross-entropy loss between $y$ and $\hat y$; i.e., show that

$$
-\sum_{w \in Vocab} y_{w} \log \left(\hat{y}_{w}\right)=-\log \left(\hat{y}_{o}\right)
$$

Your answer should be one line.
```

**Answer a** : 

å› ä¸º $\textbf{y}$ æ˜¯ç‹¬çƒ­å‘é‡ï¼Œæ‰€ä»¥ $-\sum_{w \in Vocab} y_{w} \log (\hat{y}_{w})=-y_o\log (\hat{y}_{o}) -\sum_{w \in Vocab,w \neq o} y_{w} \log (\hat{y}_{w}) = -\log (\hat{y}_{o})$ 

!!! question "Question b"

```
Compute the partial derivative of $J_{\text{naive-softmax}}(v_c, o, \textbf{U})$ with respect to $v_c$. Please write your answer in terms of $\textbf{y}, \hat {\textbf{y}}, \textbf{U}$.
```

**Answer b** : 
$$
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}} &={-\frac{\partial\left(u_{o}^{T} v_{c}\right)}{\partial v_{c}}+\frac{\partial \left(\log \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)\right)}{\partial v_{c}}} 
\\ &={-u_{o}+\frac{1}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} \frac{\partial \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)}{\partial v_{c}}
\\ &={-u_{o}+\sum_{w} \frac{\exp \left(u_{w}^{T} v_{c}\right)u_w}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} 
\\ &={-u_{o}+\sum_{w} p(O=w | C=c)u_{w}}
\\ &={-u_{o}+\sum_{w} \hat{y_w} u_{w}}
\\ &={U(\hat{y}-y)}
\end{array}
$$
!!! question "Question c"

```
Compute the partial derivatives of $J_{\text{naive-softmax}}(v_c, o, \textbf{U})$ with respect to each of the â€˜outside' word vectors, $u_w$'s. There will be two cases: when $w = o$, the true â€˜outside' word vector, and $w \neq o$, for all other words. Please write you answer in terms of $\textbf{y}, \hat {\textbf{y}}, \textbf{U}$.
```

**Answer c** : 
$$
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&={-\frac{\partial\left(u_{o}^{T} v_{c}\right)}{\partial u_{w}}+\frac{\partial \left(\log \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)\right)}{\partial u_{w}}} 
\end{array}
$$
When $w \neq o$ :
$$
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&= 0 + p(O=w | C=c) v_{c}
\\ &=\hat{y}_{w} v_{c}
\end{array}
$$
When $w = o$ :
$$
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&= -v_c + p(O=o | C=c) v_{c}
\\ &=\hat{y}_{w} v_{c} - v_c
\\ &=(\hat{y}_{w} - 1)v_c
\end{array}
$$
Then : 
$$
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} = v_c(\hat y - y)^T
$$
!!! question "Question d"

```
The sigmoid function is given by the follow Equation :

$$
\sigma(x)=\frac{1}{1+e^{-x}}=\frac{e^{x}}{e^{x}+1}
$$

Please compute the derivative of $\sigma (x)$ with respect to $x$, where $x$ is a vector.
```

**Answer d** : 
$$
\begin{array}{l}
\frac{\partial \sigma(x_i)}{\partial x_i}
&=\frac{1}{(1+\exp (-x_i))^{2}} \exp (-x_i)=\sigma(x_i)(1-\sigma(x_i)) \\
\frac{\partial \sigma(x)}{\partial x}
&= \left[\frac{\partial \sigma\left(x_{j}\right)}{\partial x_{i}}\right]_{d \times d}
\\ &=\left[\begin{array}{cccc}{\sigma^{\prime}\left(x_{1}\right)} & {0} & {\cdots} & {0} \\ {0} & {\sigma^{\prime}\left(x_{2}\right)} & {\cdots} & {0} \\ {\vdots} & {\vdots} & {\vdots} & {\vdots} \\ {0} & {0} & {\cdots} & {\sigma^{\prime}\left(x_{d}\right)}\end{array}\right]
\\ &=\text{diag}(\sigma^\prime(x))
\end{array}
$$
!!! question "Question e"

```
Now we shall consider the Negative Sampling loss, which is an alternative to the Naive
Softmax loss. Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity
of notation we shall refer to them as $w_{1}, w_{2}, \dots, w_{K}$ and their outside vectors as $u_{1}, \dots, u_{K}$. Note that
$o \notin\left\{w_{1}, \dots, w_{K}\right\}$. For a center word $c$ and an outside word $o$, the negative sampling loss function is
given by:

$$
J_{\text { neg-sample }}\left(v_{c}, o, U\right)=-\log \left(\sigma\left(u_{o}^{\top} v_{c}\right)\right)-\sum_{k=1}^{K} \log \left(\sigma\left(-u_{k}^{\top} v_{c}\right)\right)
$$

for a sample $w_{1}, w_{2}, \dots, w_{K}$, where $\sigma(\cdot)$ is the sigmoid function.

Please repeat parts b and c, computing the partial derivatives of $J_{\text { neg-sample }}$ respect to $v_c$, with
respect to $u_o$, and with respect to a negative sample $u_k$. Please write your answers in terms of the
vectors $u_o, v_c,$ and $u_k$, where $k \in[1, K]$. After you've done this, describe with one sentence why this
loss function is much more efficient to compute than the naive-softmax loss. Note, you should be able
to use your solution to part (d) to help compute the necessary gradients here.
```

**Answer e** : 

For $v_c$ :
$$
\begin{array}{l}
\frac{\partial J_{\text {neg-sample}}}{\partial v_c}
&=(\sigma(u_o^T v_c) - 1) u_o	+ \sum_{k=1}^{K}\left(1-\sigma\left(-u_{k}^{T} v_{c}\right)\right) u_{k} 
\\ &= (\sigma(u_o^T v_c) - 1) u_o+ \sum_{k=1}^{K}\sigma\left(u_{k}^{T} v_{c}\right) u_{k}
\end{array}
$$
For $u_o$, Remeber : $o \notin\left\{w_{1}, \dots, w_{K}\right\}$ ğŸ˜¢ :
$$
\frac{\partial J_{\text {neg-sample}}}{\partial u_o}=(\sigma(u_o^T v_c) - 1)v_c
$$
For $u_k$ :
$$
\frac{\partial J}{\partial \boldsymbol{u}_{k}}=-\left(\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} = \sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\boldsymbol{v}_{c}, \quad for\ k=1,2, \ldots, K
$$
Why this
loss function is much more efficient to compute than the naive-softmax loss?

For naive softmax loss function:
$$
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}}  
&={U(\hat{y}-y)}
\\ {\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} 
&= v_c(\hat y - y)^T
\end{array}
$$
For negative sampling loss function:
$$
\begin{aligned} \frac{\partial J}{\partial \boldsymbol{v}_{c}} &=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} v_{c}\right)-1\right) \boldsymbol{u}_{o} + \sum_{k=1}^{K}\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{u}_{k} 
=\sigma\left(-\boldsymbol{u}_{o}^{\top} v_{c}\right) \boldsymbol{u}_{o} + \sum_{k=1}^{K}\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{u}_{k}
\\ \frac{\partial J}{\partial \boldsymbol{u}_{o}} &=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} 
= \sigma\left(-\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\boldsymbol{v}_{c} 
\\ \frac{\partial J}{\partial \boldsymbol{u}_{k}} &=\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{v}_{c}, \quad \text { for all } k=1,2, \ldots, K \end{aligned}
$$
ä»æ±‚å¾—çš„åå¯¼æ•°ä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼ŒåŸå§‹çš„softmaxå‡½æ•°æ¯æ¬¡å¯¹ $v_c$ è¿›è¡Œåå‘ä¼ æ’­æ—¶ï¼Œéœ€è¦ä¸ output vector matrix è¿›è¡Œå¤§é‡ä¸”å¤æ‚çš„çŸ©é˜µè¿ç®—ï¼Œè€Œè´Ÿé‡‡æ ·ä¸­çš„è®¡ç®—å¤æ‚åº¦åˆ™ä¸å†ä¸è¯è¡¨å¤§å° $V$ æœ‰å…³ï¼Œè€Œæ˜¯ä¸é‡‡æ ·æ•°é‡ $K$ æœ‰å…³ã€‚

!!! question "Question f"

```
Suppose the center word is $c = w_t$ and the context window is $\left[w_{t-m}, \ldots, w_{t-1}, w_{t}, w_{t+1}, \dots,w_{t+m} \right]$, where $m$ is the context window size. Recall that for the skip-gram version of **word2vec**, the
total loss for the context window is

$$
J_{\text { skip-gram }}\left(v_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right)=\sum_{-m \leq j \leq m \atop j \neq 0} \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)
$$

Here, $J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)$ represents an arbitrary loss term for the center word $c = w_t$ and outside word
$w_t+j$ . $J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)$ could be $J_{\text {naive-softmax}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)$ or $J_{\text {neg-sample}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)$, depending on your
implementation.

Write down three partial derivatives:

$$
\partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{U} \\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{c}
\\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{w} \text { when } w \neq c
$$

Write your answers in terms of $\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{U}$ and $\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{v_c}$. This is very simple -
each solution should be one line.

***Once you're done***: Given that you computed the derivatives of $\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)$ with respect to all the
model parameters $U$ and $V$ in parts a to c, you have now computed the derivatives of the full loss
function $J_{skip-gram}$ with respect to all parameters. You're ready to implement ***word2vec*** !
```

**Answer f** : 
$$
\begin{array}{l} 
\frac{\partial J_{s g}}{\partial U} &= \sum_{-m \leq j \leq m, j \neq 0} \frac{\partial J\left(v_{c}, w_{t+j}, U\right)}{\partial U} 
\\ \frac{\partial J_{s g}}{\partial v_{c}}&= \sum_{-m \leq j \leq m, j \neq 0} \frac{\partial J\left(v_{c}, w_{t+j}, U\right)}{\partial v_{c}} 
\\ \frac{\partial J_{s g}}{\partial v_{w}}&=0(\text { when } w \neq c) \end{array}
$$

### 2 Coding: Implementing word2vec

#### word2vec.py

æœ¬éƒ¨åˆ†è¦æ±‚å®ç° $sigmoid, naiveSoftmaxLossAndGradient, negSamplingLossAndGradient, skipgram$  å››ä¸ªå‡½æ•°ï¼Œä¸»è¦è€ƒå¯Ÿå¯¹ç¬¬ä¸€éƒ¨åˆ†ä¸­åå‘ä¼ æ’­è®¡ç®—ç»“æœçš„å®ç°ã€‚ä»£ç å®ç°ä¸­ï¼Œé€šè¿‡ä¼˜åŒ–åå¯¼æ•°ç»“åˆåå¯¼æ•°è®¡ç®—ç»“æœä¸ $\sigma(x) + \sigma(-x) = 1$ å¯¹å…¬å¼è¿›è¡Œè½¬åŒ–ï¼Œä»è€Œå®ç°äº†å…¨çŸ¢é‡åŒ–ã€‚è¿™éƒ¨åˆ†éœ€è¦å¤§å®¶è‡ªè¡Œç»“åˆä»£ç ä¸å…¬å¼è¿›è¡Œæ¨å¯¼ã€‚

#### sgd.py

å®ç° SGD 
$$
\theta^{n e w}=\theta^{o l d} - \alpha \nabla_{\theta} J(\theta)
$$

#### run.py

é¦–å…ˆè¦è¯´æ˜çš„æ˜¯ï¼Œè¿™ä¸ªçœŸçš„è¦è·‘å¥½ä¹… ğŸ˜…

!!! question "Question"

```
Briefly explain in at most three sentences what you see in the plot.
```

ä¸Šå›¾æ˜¯ç»è¿‡è®­ç»ƒçš„è¯å‘é‡çš„å¯è§†åŒ–ã€‚æˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°ä¸€äº›æ¨¡å¼ï¼š

-   è¿‘ä¹‰è¯è¢«ç»„åˆåœ¨ä¸€èµ·ï¼Œæ¯”å¦‚ amazing å’Œ wonderfulï¼Œwoman å’Œ femaleã€‚
    -   ä½†æ˜¯ man å’Œ male å´è·ç¦»è¾ƒè¿œ
-   åä¹‰è¯å¯èƒ½å› ä¸ºç»å¸¸å±äºåŒä¸€ä¸Šä¸‹æ–‡ï¼Œå®ƒä»¬ä¹Ÿä¼šä¸åŒä¹‰è¯ä¸€èµ·å‡ºç°ï¼Œæ¯”å¦‚ enjoyable å’Œ annoyingã€‚
-   `man:king::woman:queen` ä»¥åŠ `queen:king::female:male` å½¢æˆçš„ä¸¤æ¡ç›´çº¿åŸºæœ¬å¹³è¡Œ

## Assignment 03

### 1. Machine Learning & Neural Networks

#### (a) Adam Optimizer

å›å¿†ä¸€ä¸‹æ ‡å‡†éšæœºæ¢¯åº¦ä¸‹é™çš„æ›´æ–°è§„åˆ™
$$
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}-\alpha \nabla_{\boldsymbol{\theta}}{J_{\mathrm{minibatch}}(\boldsymbol{\theta})}
$$
å…¶ä¸­ï¼Œ$\boldsymbol{\theta}$ æ˜¯åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„å‘é‡ï¼Œ$J$ æ˜¯æŸå¤±å‡½æ•°ï¼Œ$\nabla_{\boldsymbol{\theta}} J_{\mathrm{minibatch}}(\boldsymbol{\theta})$ æ˜¯å…³äºminibatchæ•°æ®ä¸Šå‚æ•°çš„æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼Œ$\alpha$ æ˜¯å­¦ä¹ ç‡ã€‚[Adam Optimization](https://arxiv.org/pdf/1412.6980.pdf)ä½¿ç”¨äº†ä¸€ä¸ªæ›´å¤æ‚çš„æ›´æ–°è§„åˆ™ï¼Œå¹¶é™„åŠ äº†ä¸¤ä¸ªæ­¥éª¤ã€‚

!!! question "Question 1.a.i"

```
é¦–å…ˆï¼ŒAdamä½¿ç”¨äº†ä¸€ä¸ªå«åš $momentum$ **åŠ¨é‡**çš„æŠ€å·§æ¥è·Ÿè¸ªæ¢¯åº¦çš„ç§»åŠ¨å¹³å‡å€¼ $m$

$$
\begin{aligned}
\mathbf{m} & \leftarrow \beta_{1} \mathbf{m}+\left(1-\beta_{1}\right) \nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta}) \\ \boldsymbol{\theta} & \leftarrow \boldsymbol{\theta}-\alpha \mathbf{m} \end{aligned}
$$

å…¶ä¸­ï¼Œ$\beta_1$ æ˜¯ä¸€ä¸ª 0 å’Œ 1 ä¹‹é—´çš„è¶…å‚æ•°(é€šå¸¸è¢«è®¾ä¸º0.9)ã€‚ç®€è¦è¯´æ˜(ä¸éœ€è¦ç”¨æ•°å­¦æ–¹æ³•è¯æ˜ï¼Œåªéœ€è¦ç›´è§‚åœ°è¯´æ˜)å¦‚ä½•ä½¿ç”¨mæ¥é˜»æ­¢æ›´æ–°å‘ç”Ÿå¤§çš„å˜åŒ–ï¼Œä»¥åŠæ€»ä½“ä¸Šä¸ºä»€ä¹ˆè¿™ç§å°å˜åŒ–å¯èƒ½æœ‰åŠ©äºå­¦ä¹ ã€‚
```

**Answer 1.a.i** : 

-   ç”±äºè¶…å‚æ•° $\beta _1$ ä¸€èˆ¬è¢«è®¾ä¸º0.9ï¼Œæ­¤æ—¶å¯¹äºç§»åŠ¨å¹³å‡çš„æ¢¯åº¦å€¼ $m$ è€Œè¨€ï¼Œä¸»è¦å—åˆ°çš„æ˜¯ä¹‹å‰æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡å€¼çš„å½±å“ï¼Œè€Œæœ¬æ¬¡è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦å°†ä¼šè¢«ç¼©æ”¾ä¸ºåŸæ¥çš„ ${1 - \beta_1}$ å€ï¼Œå³æ—¶æœ¬æ¬¡è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦å¾ˆå¤§ï¼ˆæ¢¯åº¦çˆ†ç‚¸ï¼‰ï¼Œè¿™ä¸€å½±å“ä¹Ÿä¼šè¢«å‡è½»ï¼Œä»è€Œé˜»æ­¢æ›´æ–°å‘ç”Ÿå¤§çš„å˜åŒ–ã€‚
-   é€šè¿‡å‡å°æ¢¯åº¦çš„å˜åŒ–ç¨‹åº¦ï¼Œä½¿å¾—æ¯æ¬¡çš„æ¢¯åº¦æ›´æ–°æ›´åŠ ç¨³å®šï¼Œä»è€Œä½¿æ¨¡å‹å­¦ä¹ æ›´åŠ ç¨³å®šï¼Œæ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œå¹¶ä¸”è¿™ä¹Ÿå‡æ…¢äº†å¯¹äºè¾ƒå¤§æ¢¯åº¦å€¼çš„å‚æ•°çš„æ›´æ–°é€Ÿåº¦ï¼Œä¿è¯å…¶æ›´æ–°çš„ç¨³å®šæ€§ã€‚

!!! question "Question 1.a.ii"

```
Adamè¿˜é€šè¿‡è·Ÿè¸ªæ¢¯åº¦å¹³æ–¹çš„ç§»åŠ¨å¹³å‡å€¼ $v$ æ¥ä½¿ç”¨è‡ªé€‚åº”å­¦ä¹ ç‡

$$
\begin{aligned} 
\mathbf{m} & \leftarrow \beta_{1} \mathbf{m}+\left(1-\beta_{1}\right) \nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta}) \\ 
\mathbf{v} & \leftarrow \beta_{2} \mathbf{v}+\left(1-\beta_{2}\right)\left(\nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta}) \odot \nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta})\right) \\ 
\boldsymbol{\theta} & \leftarrow \boldsymbol{\theta}-\alpha \odot \mathbf{m} / \sqrt{\mathbf{v}}  \end{aligned}
$$

å…¶ä¸­ï¼Œ$\odot, /$ åˆ†åˆ«è¡¨ç¤ºé€å…ƒç´ çš„ä¹˜æ³•å’Œé™¤æ³•ï¼ˆæ‰€ä»¥ $z \odot z$ æ˜¯é€å…ƒç´ çš„å¹³æ–¹ï¼‰ï¼Œ$\beta_2$ æ˜¯ä¸€ä¸ª 0 å’Œ 1 ä¹‹é—´çš„è¶…å‚æ•°(é€šå¸¸è¢«è®¾ä¸º0.99)ã€‚å› ä¸ºAdamå°†æ›´æ–°é™¤ä»¥ $\sqrt v$ ï¼Œé‚£ä¹ˆå“ªä¸ªæ¨¡å‹å‚æ•°ä¼šå¾—åˆ°æ›´å¤§çš„æ›´æ–°ï¼Ÿä¸ºä»€ä¹ˆè¿™å¯¹å­¦ä¹ æœ‰å¸®åŠ©ï¼Ÿ
```

**Answer 1.a.ii** : 

-   ç§»åŠ¨å¹³å‡æ¢¯åº¦æœ€å°çš„æ¨¡å‹å‚æ•°å°†å¾—åˆ°è¾ƒå¤§çš„æ›´æ–°ã€‚
-   ä¸€æ–¹é¢ï¼Œå°†æ¢¯åº¦è¾ƒå°çš„å‚æ•°çš„æ›´æ–°å˜å¤§ï¼Œå¸®åŠ©å…¶èµ°å‡ºå±€éƒ¨æœ€ä¼˜ç‚¹ï¼ˆéç‚¹ï¼‰ï¼›å¦ä¸€æ–¹é¢ï¼Œå°†æ¢¯åº¦è¾ƒå¤§çš„å‚æ•°çš„æ›´æ–°å˜å°ï¼Œä½¿å…¶æ›´æ–°æ›´åŠ ç¨³å®šã€‚ç»“åˆä»¥ä¸Šä¸¤ä¸ªæ–¹é¢ï¼Œä½¿å­¦ä¹ æ›´åŠ å¿«é€Ÿçš„åŒæ—¶ä¹Ÿæ›´åŠ ç¨³å®šã€‚

#### (b) Dropout

[Dropout](https://www.cs.toronto.edu/Ëœhinton/absps/JMLRdropout.pdf) æ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ã€‚åœ¨è®­ç»ƒæœŸé—´ï¼ŒDropout ä»¥ $p_{drop}$ çš„æ¦‚ç‡éšæœºè®¾ç½®éšè—å±‚ $h$ ä¸­çš„ç¥ç»å…ƒä¸ºé›¶(æ¯ä¸ªminibatchä¸­ dropout ä¸åŒçš„ç¥ç»å…ƒ),ç„¶åå°† $h$ ä¹˜ä»¥ä¸€ä¸ªå¸¸æ•° $\gamma$ ã€‚æˆ‘ä»¬å¯ä»¥å†™ä¸º
$$
\mathbf{h}_{\mathrm{drop}}=\gamma \mathbf{d} \circ \mathbf{h}
$$
å…¶ä¸­ï¼Œ$d \in \{0,1\}^{D_h}$ ( $D_h$ æ˜¯ $h$ çš„å¤§å°)æ˜¯ä¸€ä¸ªæ©ç å‘é‡ï¼Œå…¶ä¸­æ¯ä¸ªæ¡ç›®éƒ½æ˜¯ä»¥ $p_{drop}$ çš„æ¦‚ç‡ä¸º 0 ï¼Œä»¥ $1 - p_{drop}$ çš„æ¦‚ç‡ä¸º 1ã€‚$\gamma$ æ˜¯ä½¿å¾— $h_{drop}$ çš„æœŸæœ›å€¼ä¸º $h$ çš„å€¼
$$
\mathbb{E}_{p_{\text{drop}}}\left[\mathbf{h}_{\text{drop}}\right]_{i}=h_{i}, \text{for all } i \in \{1,\dots,D_h\}
$$
!!! question "Question 1.b.i"

```
$\gamma$ å¿…é¡»ç­‰äºä»€ä¹ˆ(ç”¨ $p_{drop}$ è¡¨ç¤º) ï¼Ÿç®€å•è¯æ˜ä½ çš„ç­”æ¡ˆã€‚
```

**Answer 1.b.i** : 
$$
\gamma = \frac{1}{1 - p_{drop}} \\
$$
è¯æ˜å¦‚ä¸‹ï¼š
$$
\sum_i (1 -  p_{drop}) h_i = (1 -  p_{drop}) E[h] \\
\sum_i[h_{drop}]_i = \gamma\sum_i (1 -  p_{drop}) h_i = \gamma (1 -  p_{drop}) E[h] = E[h]
$$
!!! question "Question 1.b.ii"

```
ä¸ºä»€ä¹ˆæˆ‘ä»¬åº”è¯¥åªåœ¨è®­ç»ƒæ—¶ä½¿ç”¨ dropout è€Œåœ¨è¯„ä¼°æ—¶ä¸ä½¿ç”¨ï¼Ÿ
```

**Answer 1.b.ii** : 

å¦‚æœæˆ‘ä»¬åœ¨è¯„ä¼°æœŸé—´åº”ç”¨ dropout ï¼Œé‚£ä¹ˆè¯„ä¼°ç»“æœå°†ä¼šå…·æœ‰éšæœºæ€§ï¼Œå¹¶ä¸èƒ½ä½“ç°æ¨¡å‹çš„çœŸå®æ€§èƒ½ï¼Œè¿èƒŒäº†æ­£åˆ™åŒ–çš„åˆè¡·ã€‚é€šè¿‡åœ¨è¯„ä¼°æœŸé—´ç¦ç”¨ dropoutï¼Œä»è€Œè§‚å¯Ÿæ¨¡å‹çš„æ€§èƒ½ä¸æ­£åˆ™åŒ–çš„æ•ˆæœï¼Œä¿è¯æ¨¡å‹çš„å‚æ•°å¾—åˆ°æ­£ç¡®çš„æ›´æ–°ã€‚

### 2. Neural Transition-Based Dependency Parsing

åœ¨æœ¬èŠ‚ä¸­ï¼Œæ‚¨å°†å®ç°ä¸€ä¸ªåŸºäºç¥ç»ç½‘ç»œçš„ä¾èµ–è§£æå™¨ï¼Œå…¶ç›®æ ‡æ˜¯åœ¨UAS(æœªæ ‡è®°ä¾å­˜è¯„åˆ†)æŒ‡æ ‡ä¸Šæœ€å¤§åŒ–æ€§èƒ½ã€‚

ä¾å­˜è§£æå™¨åˆ†æå¥å­çš„è¯­æ³•ç»“æ„ï¼Œåœ¨ head words å’Œ ä¿®é¥° head words çš„å•è¯ä¹‹é—´å»ºç«‹å…³ç³»ã€‚ä½ çš„å®ç°å°†æ˜¯ä¸€ä¸ªåŸºäºè½¬æ¢çš„è§£æå™¨ï¼Œå®ƒé€æ­¥æ„å»ºä¸€ä¸ªè§£æã€‚æ¯ä¸€æ­¥éƒ½ç»´æŠ¤ä¸€ä¸ªå±€éƒ¨è§£æï¼Œè¡¨ç¤ºå¦‚ä¸‹

-   ä¸€ä¸ªå­˜å‚¨æ­£åœ¨è¢«å¤„ç†çš„å•è¯çš„ æ ˆ 
-   ä¸€ä¸ªå­˜å‚¨å°šæœªå¤„ç†çš„å•è¯çš„ ç¼“å­˜
-   ä¸€ä¸ªè§£æå™¨é¢„æµ‹çš„ ä¾èµ– çš„åˆ—è¡¨

æœ€åˆ,æ ˆåªåŒ…å« ROOT ï¼Œä¾èµ–é¡¹åˆ—è¡¨æ˜¯ç©ºçš„ï¼Œè€Œç¼“å­˜åˆ™åŒ…å«äº†è¿™ä¸ªå¥å­çš„æ‰€æœ‰å•è¯ã€‚åœ¨æ¯ä¸€ä¸ªæ­¥éª¤ä¸­,è§£æå™¨å°†å¯¹éƒ¨åˆ†è§£æä½¿ç”¨ä¸€ä¸ªè½¬æ¢,ç›´åˆ°å®ƒçš„é­‚æ‘æ˜¯ç©ºçš„ï¼Œå¹¶ä¸”æ ˆå¤§å°ä¸º1ã€‚å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è½¬æ¢ï¼š

-   SHIFTï¼šå°†bufferä¸­çš„ç¬¬ä¸€ä¸ªè¯ç§»å‡ºå¹¶æ”¾åˆ°stackä¸Šã€‚
-   LEFT-ARCï¼šå°†ç¬¬äºŒä¸ª(æœ€è¿‘æ·»åŠ çš„ç¬¬äºŒ)é¡¹æ ‡è®°ä¸ºæ ˆé¡¶å…ƒç´ çš„ä¾èµ–ï¼Œå¹¶ä»å †æ ˆä¸­åˆ é™¤ç¬¬äºŒé¡¹
-   RIGHT-ARCï¼šå°†ç¬¬ä¸€ä¸ª(æœ€è¿‘æ·»åŠ çš„ç¬¬ä¸€)é¡¹æ ‡è®°ä¸ºæ ˆä¸­ç¬¬äºŒé¡¹çš„ä¾èµ–ï¼Œå¹¶ä»å †æ ˆä¸­åˆ é™¤ç¬¬ä¸€é¡¹

åœ¨æ¯ä¸ªæ­¥éª¤ä¸­ï¼Œè§£æå™¨å°†ä½¿ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œåˆ†ç±»å™¨åœ¨ä¸‰ä¸ªè½¬æ¢ä¸­å†³å®šã€‚

!!! question "Question 2.a"

```
æ±‚è§£è§£æå¥å­ â€œI parsed this sentence correctlyâ€ æ‰€éœ€çš„è½¬æ¢é¡ºåºã€‚è¿™å¥è¯çš„ä¾èµ–æ ‘å¦‚ä¸‹æ‰€ç¤ºã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œç»™å‡º stack å’Œ buffer çš„ç»“æ„ï¼Œä»¥åŠæœ¬æ­¥éª¤åº”ç”¨äº†ä»€ä¹ˆè½¬æ¢ï¼Œå¹¶æ·»åŠ æ–°çš„ä¾èµ–(å¦‚æœæœ‰çš„è¯)ã€‚ä¸‹é¢æä¾›äº†ä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ã€‚

![1560871900131](imgs/1560871900131.png)
```

**Answer 2.a** : 

| Stack                          | Buffer                                 | New dependency         | Transition           |
| ------------------------------ | -------------------------------------- | ---------------------- | -------------------- |
| [ROOT]                         | [I, parsed, this, sentence, correctly] |                        | Initial Conï¬guration |
| [ROOT, I]                      | [parsed, this, sentence, correctly]    |                        | SHIFT                |
| [ROOT, I, parsed]              | [this, sentence, correctly]            |                        | SHIFT                |
| [ROOT, parsed]                 | [this, sentence, correctly]            | parsed $\to$ I         | LEFT-ARC             |
| [ROOT, parsed, this]           | [sentence, correctly]                  |                        | SHIFT                |
| [ROOT, parsed, this, sentence] | [correctly]                            |                        | SHIFT                |
| [ROOT, parsed, sentence]       | [correctly]                            | sentence $\to$ this    | LEFT-ARC             |
| [ROOT, parsed]                 | [correctly]                            | parsed $\to$ sentence  | RIGHT-ARC            |
| [ROOT, parsed, correctly]      | []                                     |                        | SHIFT                |
| [ROOT, parsed]                 | []                                     | parsed $\to$ correctly | RIGHT-ARC            |
| [ROOT]                         | []                                     | ROOT $\to$ parsed      | RIGHT-ARC            |

!!! question "Question 2.b"

```
ä¸€ä¸ªåŒ…å« $n$ ä¸ªå•è¯çš„å¥å­éœ€è¦å¤šå°‘æ­¥(ç”¨ $n$ è¡¨ç¤º)æ‰èƒ½è¢«è§£æï¼Ÿç®€è¦è§£é‡Šä¸ºä»€ä¹ˆã€‚
```

**Answer 2.b** : 

åŒ…å«$n$ä¸ªå•è¯çš„å¥å­éœ€è¦ $2 \times n$ æ­¥æ‰èƒ½å®Œæˆè§£æã€‚å› ä¸ºéœ€è¦è¿›è¡Œ $n$ æ­¥çš„ $SHIFT$ æ“ä½œå’Œ å…±è®¡$n æ­¥çš„ LEFT-ARC æˆ– RIGHT-ARC æ“ä½œï¼Œæ‰èƒ½å®Œæˆè§£æã€‚ï¼ˆæ¯ä¸ªå•è¯éƒ½éœ€è¦ä¸€æ¬¡SHIFTå’ŒARCçš„æ“ä½œï¼Œåˆå§‹åŒ–æ­¥éª¤ä¸è®¡ç®—åœ¨å†…ï¼‰

**Question 2.c**

å®ç°è§£æå™¨å°†ä½¿ç”¨çš„è½¬æ¢æœºåˆ¶

**Question 2.d**

æˆ‘ä»¬çš„ç½‘ç»œå°†é¢„æµ‹å“ªäº›è½¬æ¢åº”è¯¥åº”ç”¨äºéƒ¨åˆ†è§£æã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥è§£æä¸€ä¸ªå¥å­ï¼Œé€šè¿‡åº”ç”¨é¢„æµ‹å‡ºçš„è½¬æ¢æ“ä½œï¼Œç›´åˆ°è§£æå®Œæˆã€‚ç„¶è€Œï¼Œåœ¨å¯¹å¤§é‡æ•°æ®è¿›è¡Œé¢„æµ‹æ—¶ï¼Œç¥ç»ç½‘ç»œçš„è¿è¡Œé€Ÿåº¦è¦é«˜å¾—å¤š(å³åŒæ—¶é¢„æµ‹äº†å¯¹ä»»ä½•ä¸åŒéƒ¨åˆ†è§£æçš„ä¸‹ä¸€ä¸ªè½¬æ¢)ã€‚æˆ‘ä»¬å¯ä»¥ç”¨ä¸‹é¢çš„ç®—æ³•æ¥è§£æå°æ‰¹æ¬¡çš„å¥å­

![1560906831993](imgs/1560906831993.png)

å®ç°minibatchçš„è§£æå™¨

æˆ‘ä»¬ç°åœ¨å°†è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œæ¥é¢„æµ‹ï¼Œè€ƒè™‘åˆ°æ ˆã€ç¼“å­˜å’Œä¾èµ–é¡¹é›†åˆçš„çŠ¶æ€ï¼Œä¸‹ä¸€æ­¥åº”è¯¥åº”ç”¨å“ªä¸ªè½¬æ¢ã€‚é¦–å…ˆï¼Œæ¨¡å‹æå–äº†ä¸€ä¸ªè¡¨ç¤ºå½“å‰çŠ¶æ€çš„ç‰¹å¾å‘é‡ã€‚æˆ‘ä»¬å°†ä½¿ç”¨åŸç¥ç»ä¾èµ–è§£æè®ºæ–‡ä¸­çš„ç‰¹å¾é›†åˆï¼š[A Fast and Accurate Dependency Parser using Neural Networks](http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf)ã€‚è¿™ä¸ªç‰¹å¾å‘é‡ç”±æ ‡è®°åˆ—è¡¨(ä¾‹å¦‚åœ¨æ ˆä¸­çš„æœ€åä¸€ä¸ªè¯ï¼Œç¼“å­˜ä¸­çš„ç¬¬ä¸€ä¸ªè¯ï¼Œæ ˆä¸­ç¬¬äºŒåˆ°æœ€åä¸€ä¸ªå­—çš„ä¾èµ–(å¦‚æœæœ‰))ç»„æˆã€‚å®ƒä»¬å¯ä»¥è¢«è¡¨ç¤ºä¸ºæ•´æ•°çš„åˆ—è¡¨$[w_1,w_2,\dots,w_m]$ï¼Œmæ˜¯ç‰¹å¾çš„æ•°é‡ï¼Œæ¯ä¸ª $0 \leq w_i \lt |V|$ æ˜¯è¯æ±‡è¡¨ä¸­çš„ä¸€ä¸ªtokençš„ç´¢å¼•($| V |$æ˜¯è¯æ±‡é‡)ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„ç½‘ç»œæŸ¥æ‰¾æ¯ä¸ªå•è¯çš„åµŒå…¥ï¼Œå¹¶å°†å®ƒä»¬è¿æ¥æˆä¸€ä¸ªè¾“å…¥å‘é‡ï¼š
$$
\mathbf{x}=\left[\mathbf{E}_{w_{1}}, \dots, \mathbf{E}_{w_{m}}\right] \in \mathbb{R}^{d m}
$$
å…¶ä¸­ $\mathbf{E} \in \mathbb{R}^{|V| \times d}$ æ˜¯åµŒå…¥çŸ©é˜µï¼Œæ¯ä¸€è¡Œ $\mathbf{E}_w$ æ˜¯ä¸€ä¸ªç‰¹å®šçš„å•è¯ $w$ çš„å‘é‡ã€‚æ¥ç€æˆ‘ä»¬å¯ä»¥è®¡ç®—æˆ‘ä»¬çš„é¢„æµ‹ï¼š
$$
\mathbf h = \text{ReLU}(\mathbf{xW+b_1}) \\
\mathbf l = \text{ReLU}(\mathbf{hU+b_2}) \\
\mathbf {\hat y} = \text{softmax}(l) 
$$
å…¶ä¸­ï¼Œ $\mathbf{h}$ æŒ‡çš„æ˜¯éšè—å±‚ï¼Œ$\mathbf{l}$ æ˜¯å…¶åˆ†æ•°ï¼Œ$\mathbf{\hat y}$ æŒ‡çš„æ˜¯é¢„æµ‹ç»“æœï¼Œ $\text{ReLU(z)}=max(z,0)$ ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€å°åŒ–äº¤å‰ç†µæŸå¤±æ¥è®­ç»ƒæ¨¡å‹
$$
J(\theta) = CE(\mathbf y,\mathbf{\hat y}) = -\sum^3_{i=1}y_i\log\hat y_i
$$
è®­ç»ƒé›†çš„æŸå¤±ä¸ºæ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„ $J(\theta)$ çš„å¹³å‡å€¼ã€‚

**Question 2.f**

æˆ‘ä»¬æƒ³çœ‹çœ‹ä¾èµ–å…³ç³»è§£æçš„ä¾‹å­ï¼Œå¹¶äº†è§£åƒæˆ‘ä»¬è¿™æ ·çš„è§£æå™¨åœ¨ä»€ä¹ˆåœ°æ–¹å¯èƒ½æ˜¯é”™è¯¯çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿™ä¸ªå¥å­ä¸­:

![1560950604163](imgs/1560950604163.png)

ä¾èµ– $\text{into Afghanistan}$ æ˜¯é”™çš„ï¼Œå› ä¸ºè¿™ä¸ªçŸ­è¯­åº”è¯¥ä¿®é¥° $\text{sent}$ (ä¾‹å¦‚ $\text{sent into Afghanistan}$) è€Œä¸æ˜¯ $\text{troops}$ (å› ä¸º $\text{ troops into Afghanistan}$ æ²¡æœ‰æ„ä¹‰)ã€‚ä¸‹é¢æ˜¯æ­£ç¡®çš„è§£æï¼š

![1560950910787](imgs/1560950910787.png)

ä¸€èˆ¬æ¥è¯´ï¼Œä»¥ä¸‹æ˜¯å››ç§è§£æé”™è¯¯ï¼š

-   **Prepositional Phrase Attachment Error** ä»‹è¯çŸ­è¯­è¿æ¥é”™è¯¯ï¼šåœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œè¯ç»„ $\text{into Afghanistan}$ æ˜¯ä¸€ä¸ªä»‹è¯çŸ­è¯­ã€‚ä»‹è¯çŸ­è¯­è¿æ¥é”™è¯¯æ˜¯æŒ‡ä»‹è¯çŸ­è¯­è¿æ¥åˆ°é”™è¯¯çš„ head word ä¸Š(åœ¨æœ¬ä¾‹ä¸­ï¼Œtroops æ˜¯é”™è¯¯çš„ head word ï¼Œsent æ˜¯æ­£ç¡®çš„ head word )ã€‚ä»‹è¯çŸ­è¯­çš„æ›´å¤šä¾‹å­åŒ…æ‹¬with a rock, before midnightå’Œunder the carpetã€‚
-   **Verb Phrase Attachment Error** åŠ¨è¯çŸ­è¯­è¿æ¥é”™è¯¯ï¼šåœ¨å¥å­$\text{leave the store alone, I went out to watch the parade}$ä¸­ï¼ŒçŸ­è¯­ $\text{leave the store alone}$ æ˜¯åŠ¨è¯çŸ­è¯­ã€‚åŠ¨è¯çŸ­è¯­è¿æ¥é”™è¯¯æ˜¯æŒ‡ä¸€ä¸ªåŠ¨è¯çŸ­è¯­è¿æ¥åˆ°é”™è¯¯çš„ head word ä¸Š(åœ¨æœ¬ä¾‹ä¸­ï¼Œæ­£ç¡®çš„å¤´è¯æ˜¯ $\text{went}$)ã€‚
-   **Modiï¬er Attachment Error** ä¿®é¥°è¯­è¿æ¥é”™è¯¯ï¼šåœ¨å¥å­ $\text{I am extremely short}$ ä¸­ï¼Œå‰¯è¯extremely æ˜¯å½¢å®¹è¯ short çš„ä¿®é¥°è¯­ã€‚ä¿®é¥°è¯­é™„åŠ é”™è¯¯æ˜¯ä¿®é¥°è¯­é™„åŠ åˆ°é”™è¯¯çš„ head word ä¸Šæ—¶å‘ç”Ÿçš„é”™è¯¯(åœ¨æœ¬ä¾‹ä¸­ï¼Œæ­£ç¡®çš„å¤´è¯æ˜¯ short)ã€‚
-   **Coordination Attachment Error** åè°ƒè¿æ¥é”™è¯¯ï¼šåœ¨å¥å­ $\text{Would you like brown rice or garlic naan?}$ ä¸­ï¼Œ brown rice å’Œgarlic naanéƒ½æ˜¯è¿è¯ï¼Œoræ˜¯å¹¶åˆ—è¿è¯ã€‚ç¬¬äºŒä¸ªè¿æ¥è¯(è¿™é‡Œæ˜¯garlic naan)åº”è¯¥è¿æ¥åˆ°ç¬¬ä¸€ä¸ªè¿æ¥è¯(è¿™é‡Œæ˜¯brown rice)ã€‚åè°ƒè¿æ¥é”™è¯¯æ˜¯å½“ç¬¬äºŒä¸ªè¿æ¥è¯é™„åŠ åˆ°é”™è¯¯çš„ head word ä¸Šæ—¶(åœ¨æœ¬ä¾‹ä¸­ï¼Œæ­£ç¡®çš„å¤´è¯æ˜¯rice)ã€‚å…¶ä»–å¹¶åˆ—è¿è¯åŒ…æ‹¬and, butå’Œsoã€‚

åœ¨è¿™ä¸ªé—®é¢˜ä¸­æœ‰å››ä¸ªå¥å­ï¼Œå…¶ä¸­åŒ…å«ä»è§£æå™¨è·å¾—çš„ä¾èµ–é¡¹è§£æã€‚æ¯ä¸ªå¥å­éƒ½æœ‰ä¸€ä¸ªé”™è¯¯ï¼Œä¸Šé¢å››ç§ç±»å‹éƒ½æœ‰ä¸€ä¸ªä¾‹å­ã€‚å¯¹äºæ¯ä¸ªå¥å­ï¼Œè¯·è¯´æ˜é”™è¯¯çš„ç±»å‹ã€ä¸æ­£ç¡®çš„ä¾èµ–é¡¹å’Œæ­£ç¡®çš„ä¾èµ–é¡¹ã€‚ä¸ºäº†æ¼”ç¤º:å¯¹äºä¸Šé¢çš„ä¾‹å­ï¼Œæ‚¨å¯ä»¥è¿™æ ·å†™ï¼š

-   Error type: Prepositional Phrase Attachment Error 
-   Incorrect dependency: troops $\to$ Afghanistan
-   Correct dependency: sent $\to$ Afghanistan 

æ³¨æ„ï¼šä¾èµ–é¡¹æ³¨é‡Šæœ‰å¾ˆå¤šç»†èŠ‚å’Œçº¦å®šã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºä»–ä»¬çš„ä¿¡æ¯ï¼Œä½ å¯ä»¥æµè§ˆUDç½‘ç«™:http://universaldependencies.orgã€‚ç„¶è€Œï¼Œä½ ä¸éœ€è¦çŸ¥é“æ‰€æœ‰è¿™äº›ç»†èŠ‚å°±èƒ½å›ç­”è¿™ä¸ªé—®é¢˜ã€‚åœ¨æ¯ä¸€ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½åœ¨è¯¢é—®çŸ­è¯­çš„è¿æ¥ï¼Œåº”è¯¥è¶³ä»¥çœ‹å‡ºå®ƒä»¬æ˜¯å¦ä¿®é¥°äº†æ­£ç¡®çš„headã€‚ç‰¹åˆ«æ˜¯ï¼Œä½ ä¸éœ€è¦æŸ¥çœ‹ä¾èµ–é¡¹è¾¹ç¼˜ä¸Šçš„æ ‡ç­¾â€”â€”åªéœ€æŸ¥çœ‹è¾¹ç¼˜æœ¬èº«å°±è¶³å¤Ÿäº†ã€‚

**Answer 2.f**

![1560951554929](imgs/1560951554929.png)

-   Error type: Verb Phrase Attachment Error
-   Incorrect dependency: wedding $\to$ fearing
-   Correct dependency: heading $\to$ fearing

![1560951560930](imgs/1560951560930.png)

-   Error type: Coordination Attachment Error
-   Incorrect dependency: makes $\to$ rescue
-   Correct dependency: rush $\to$ rescue

![1560951569847](imgs/1560951569847.png)

-   Error type: Prepositional Phrase Attachment Error
-   Incorrect dependency: named $\to$ Midland
-   Correct dependency: guy $\to$ Midland

![1560951576042](imgs/1560951576042.png)

-   Error type: Modiï¬er Attachment Error 
-   Incorrect dependency: elements $\to$ most
-   Correct dependency: crucial $\to$ most

## Reference

-   [ä»SVDåˆ°PCAâ€”â€”å¥‡å¦™çš„æ•°å­¦æ¸¸æˆ](

-   <https://my.oschina.net/findbill/blog/535044>)