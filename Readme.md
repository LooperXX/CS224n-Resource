# CS224n-2019

æœ¬`Repo`ä¸»è¦åŒ…æ‹¬è¯¾ç¨‹çš„ä½œä¸šä¸æ–‡æ¡£(Lecture, Note, Additional Readings, Suggested Readings)

è¯¾ç¨‹ç¬”è®°å‚è§æˆ‘çš„[åšå®¢](https://looperxx.github.io/CS224n-2019-01-Introduction%20and%20Word%20Vectors/)ï¼Œå¹¶åœ¨åšå®¢çš„[Repo](<https://github.com/LooperXX/LooperXX.github.io>)ä¸­æä¾›ç¬”è®°æºæ–‡ä»¶çš„ä¸‹è½½

æœ¬ç¬”è®°ä¸ä»…ä½¿ç”¨äº†Markdownçš„é€šç”¨è¯­æ³•ï¼Œè¿˜ä½¿ç”¨äº† mkdocs-material çš„ä¸€äº›è¯­æ³•ä»¥æå‡è§†è§‰æ•ˆæœï¼Œæ¨èé€šè¿‡æˆ‘çš„[åšå®¢](<https://looperxx.github.io/CS224n-2019-Assignment/>)è¿›è¡Œè®¿é—®ã€‚

## Assignment

æœ¬æ–‡æ¡£å°†ç®€è¦è®°å½•ä½œä¸šä¸­çš„è¦ç‚¹

### Assignment 01

-   é€æ­¥å®Œæˆå…±ç°çŸ©é˜µçš„æ­å»ºï¼Œå¹¶è°ƒç”¨ `sklearn.decomposition` ä¸­çš„ `TruncatedSVD` å®Œæˆä¼ ç»Ÿçš„åŸºäºSVDçš„é™ç»´ç®—æ³•
-   å¯è§†åŒ–å±•ç¤ºï¼Œè§‚å¯Ÿå¹¶åˆ†æå…¶åœ¨äºŒç»´ç©ºé—´ä¸‹çš„èšé›†æƒ…å†µã€‚
-   è½½å…¥Word2Vecï¼Œå°†å…¶ä¸SVDå¾—åˆ°çš„å•è¯åˆ†å¸ƒæƒ…å†µè¿›è¡Œå¯¹æ¯”ï¼Œåˆ†æä¸¤è€…è¯å‘é‡çš„ä¸åŒä¹‹å¤„ã€‚
-   å­¦ä¹ ä½¿ç”¨`gensim`ï¼Œä½¿ç”¨`Cosine Similarity` åˆ†æå•è¯çš„ç›¸ä¼¼åº¦ï¼Œå¯¹æ¯”å•è¯å’Œå…¶åŒä¹‰è¯ä¸åä¹‰è¯çš„`Cosine Distance` ï¼Œå¹¶å°è¯•æ‰¾åˆ°æ­£ç¡®çš„ä¸é”™è¯¯çš„ç±»æ¯”æ ·ä¾‹
-   æ¢å¯»Word2Vecå‘é‡ä¸­å­˜åœ¨çš„ `Independent Bias` é—®é¢˜

## Assignment 02

### 1  Written: Understanding word2vec

$$
P(O=o | C=c)=\frac{\exp \left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}{\sum_{w \in \mathrm{Vocab}} \exp \left(\boldsymbol{u}_{w}^{\top} \boldsymbol{v}_{c}\right)}
$$

$$
J_{\text { naive-softmax }}\left(v_{c}, o, U\right)=-\log P(O=o | C=c)
$$

çœŸå®(ç¦»æ•£)æ¦‚ç‡åˆ†å¸ƒ $p$ ä¸å¦ä¸€ä¸ªåˆ†å¸ƒ $q$ çš„äº¤å‰ç†µæŸå¤±ä¸º $-\sum_i p_{i} \log \left(q_{i}\right)$

!!! question "Question a"

```
Show that the naive-softmax loss given in Equation (2) is the same as the cross-entropy loss between $y$ and $\hat y$; i.e., show that

$$
-\sum_{w \in Vocab} y_{w} \log \left(\hat{y}_{w}\right)=-\log \left(\hat{y}_{o}\right)
$$

Your answer should be one line.
```

**Answer a** : 

å› ä¸º $\textbf{y}$ æ˜¯ç‹¬çƒ­å‘é‡ï¼Œæ‰€ä»¥ $-\sum_{w \in Vocab} y_{w} \log (\hat{y}_{w})=-y_o\log (\hat{y}_{o}) -\sum_{w \in Vocab,w \neq o} y_{w} \log (\hat{y}_{w}) = -\log (\hat{y}_{o})$ 

!!! question "Question b"

```
Compute the partial derivative of $J_{\text{naive-softmax}}(v_c, o, \textbf{U})$ with respect to $v_c$. Please write your answer in terms of $\textbf{y}, \hat {\textbf{y}}, \textbf{U}$.
```

**Answer b** : 
$$
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}} &={-\frac{\partial\left(u_{o}^{T} v_{c}\right)}{\partial v_{c}}+\frac{\partial \left(\log \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)\right)}{\partial v_{c}}} 
\\ &={-u_{o}+\frac{1}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} \frac{\partial \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)}{\partial v_{c}}
\\ &={-u_{o}+\sum_{w} \frac{\exp \left(u_{w}^{T} v_{c}\right)u_w}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} 
\\ &={-u_{o}+\sum_{w} p(O=w | C=c)u_{w}}
\\ &={-u_{o}+\sum_{w} \hat{y_w} u_{w}}
\\ &={U(\hat{y}-y)}
\end{array}
$$
!!! question "Question c"

```
Compute the partial derivatives of $J_{\text{naive-softmax}}(v_c, o, \textbf{U})$ with respect to each of the â€˜outside' word vectors, $u_w$'s. There will be two cases: when $w = o$, the true â€˜outside' word vector, and $w \neq o$, for all other words. Please write you answer in terms of $\textbf{y}, \hat {\textbf{y}}, \textbf{U}$.
```

**Answer c** : 
$$
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&={-\frac{\partial\left(u_{o}^{T} v_{c}\right)}{\partial u_{w}}+\frac{\partial \left(\log \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)\right)}{\partial u_{w}}} 
\end{array}
$$
When $w \neq o$ :
$$
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&= 0 + p(O=w | C=c) v_{c}
\\ &=\hat{y}_{w} v_{c}
\end{array}
$$
When $w = o$ :
$$
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&= -v_c + p(O=o | C=c) v_{c}
\\ &=\hat{y}_{w} v_{c} - v_c
\\ &=(\hat{y}_{w} - 1)v_c
\end{array}
$$
Then : 
$$
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} = v_c(\hat y - y)^T
$$
!!! question "Question d"

```
The sigmoid function is given by the follow Equation :

$$
\sigma(x)=\frac{1}{1+e^{-x}}=\frac{e^{x}}{e^{x}+1}
$$

Please compute the derivative of $\sigma (x)$ with respect to $x$, where $x$ is a vector.
```

**Answer d** : 
$$
\begin{array}{l}
\frac{\partial \sigma(x_i)}{\partial x_i}
&=\frac{1}{(1+\exp (-x_i))^{2}} \exp (-x_i)=\sigma(x_i)(1-\sigma(x_i)) \\
\frac{\partial \sigma(x)}{\partial x}
&= \left[\frac{\partial \sigma\left(x_{j}\right)}{\partial x_{i}}\right]_{d \times d}
\\ &=\left[\begin{array}{cccc}{\sigma^{\prime}\left(x_{1}\right)} & {0} & {\cdots} & {0} \\ {0} & {\sigma^{\prime}\left(x_{2}\right)} & {\cdots} & {0} \\ {\vdots} & {\vdots} & {\vdots} & {\vdots} \\ {0} & {0} & {\cdots} & {\sigma^{\prime}\left(x_{d}\right)}\end{array}\right]
\\ &=\text{diag}(\sigma^\prime(x))
\end{array}
$$
!!! question "Question e"

```
Now we shall consider the Negative Sampling loss, which is an alternative to the Naive
Softmax loss. Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity
of notation we shall refer to them as $w_{1}, w_{2}, \dots, w_{K}$ and their outside vectors as $u_{1}, \dots, u_{K}$. Note that
$o \notin\left\{w_{1}, \dots, w_{K}\right\}$. For a center word $c$ and an outside word $o$, the negative sampling loss function is
given by:

$$
J_{\text { neg-sample }}\left(v_{c}, o, U\right)=-\log \left(\sigma\left(u_{o}^{\top} v_{c}\right)\right)-\sum_{k=1}^{K} \log \left(\sigma\left(-u_{k}^{\top} v_{c}\right)\right)
$$

for a sample $w_{1}, w_{2}, \dots, w_{K}$, where $\sigma(\cdot)$ is the sigmoid function.

Please repeat parts b and c, computing the partial derivatives of $J_{\text { neg-sample }}$ respect to $v_c$, with
respect to $u_o$, and with respect to a negative sample $u_k$. Please write your answers in terms of the
vectors $u_o, v_c,$ and $u_k$, where $k \in[1, K]$. After you've done this, describe with one sentence why this
loss function is much more efficient to compute than the naive-softmax loss. Note, you should be able
to use your solution to part (d) to help compute the necessary gradients here.
```

**Answer e** : 

For $v_c$ :
$$
\begin{array}{l}
\frac{\partial J_{\text {neg-sample}}}{\partial v_c}
&=(\sigma(u_o^T v_c) - 1) u_o	+ \sum_{k=1}^{K}\left(1-\sigma\left(-u_{k}^{T} v_{c}\right)\right) u_{k} 
\\ &= (\sigma(u_o^T v_c) - 1) u_o+ \sum_{k=1}^{K}\sigma\left(u_{k}^{T} v_{c}\right) u_{k}
\end{array}
$$
For $u_o$, Remeber : $o \notin\left\{w_{1}, \dots, w_{K}\right\}$ ğŸ˜¢ :
$$
\frac{\partial J_{\text {neg-sample}}}{\partial u_o}=(\sigma(u_o^T v_c) - 1)v_c
$$
For $u_k$ :
$$
\frac{\partial J}{\partial \boldsymbol{u}_{k}}=-\left(\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} = \sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\boldsymbol{v}_{c}, \quad for\ k=1,2, \ldots, K
$$
Why this
loss function is much more efficient to compute than the naive-softmax loss?

For naive softmax loss function:
$$
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}}  
&={U(\hat{y}-y)}
\\ {\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} 
&= v_c(\hat y - y)^T
\end{array}
$$
For negative sampling loss function:
$$
\begin{aligned} \frac{\partial J}{\partial \boldsymbol{v}_{c}} &=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} v_{c}\right)-1\right) \boldsymbol{u}_{o} + \sum_{k=1}^{K}\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{u}_{k} 
=\sigma\left(-\boldsymbol{u}_{o}^{\top} v_{c}\right) \boldsymbol{u}_{o} + \sum_{k=1}^{K}\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{u}_{k}
\\ \frac{\partial J}{\partial \boldsymbol{u}_{o}} &=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} 
= \sigma\left(-\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\boldsymbol{v}_{c} 
\\ \frac{\partial J}{\partial \boldsymbol{u}_{k}} &=\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{v}_{c}, \quad \text { for all } k=1,2, \ldots, K \end{aligned}
$$
ä»æ±‚å¾—çš„åå¯¼æ•°ä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼ŒåŸå§‹çš„softmaxå‡½æ•°æ¯æ¬¡å¯¹ $v_c$ è¿›è¡Œåå‘ä¼ æ’­æ—¶ï¼Œéœ€è¦ä¸ output vector matrix è¿›è¡Œå¤§é‡ä¸”å¤æ‚çš„çŸ©é˜µè¿ç®—ï¼Œè€Œè´Ÿé‡‡æ ·ä¸­çš„è®¡ç®—å¤æ‚åº¦åˆ™ä¸å†ä¸è¯è¡¨å¤§å° $V$ æœ‰å…³ï¼Œè€Œæ˜¯ä¸é‡‡æ ·æ•°é‡ $K$ æœ‰å…³ã€‚

!!! question "Question f"

```
Suppose the center word is $c = w_t$ and the context window is $\left[w_{t-m}, \ldots, w_{t-1}, w_{t}, w_{t+1}, \dots,w_{t+m} \right]$, where $m$ is the context window size. Recall that for the skip-gram version of **word2vec**, the
total loss for the context window is

$$
J_{\text { skip-gram }}\left(v_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right)=\sum_{-m \leq j \leq m \atop j \neq 0} \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)
$$

Here, $J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)$ represents an arbitrary loss term for the center word $c = w_t$ and outside word
$w_t+j$ . $J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)$ could be $J_{\text {naive-softmax}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)$ or $J_{\text {neg-sample}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)$, depending on your
implementation.

Write down three partial derivatives:

$$
\partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{U} \\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{c}
\\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{w} \text { when } w \neq c
$$

Write your answers in terms of $\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{U}$ and $\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{v_c}$. This is very simple -
each solution should be one line.

***Once you're done***: Given that you computed the derivatives of $\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)$ with respect to all the
model parameters $U$ and $V$ in parts a to c, you have now computed the derivatives of the full loss
function $J_{skip-gram}$ with respect to all parameters. You're ready to implement ***word2vec*** !
```

**Answer f** : 
$$
\begin{array}{l} 
\frac{\partial J_{s g}}{\partial U} &= \sum_{-m \leq j \leq m, j \neq 0} \frac{\partial J\left(v_{c}, w_{t+j}, U\right)}{\partial U} 
\\ \frac{\partial J_{s g}}{\partial v_{c}}&= \sum_{-m \leq j \leq m, j \neq 0} \frac{\partial J\left(v_{c}, w_{t+j}, U\right)}{\partial v_{c}} 
\\ \frac{\partial J_{s g}}{\partial v_{w}}&=0(\text { when } w \neq c) \end{array}
$$

### 2 Coding: Implementing word2vec

#### word2vec.py

æœ¬éƒ¨åˆ†è¦æ±‚å®ç° $sigmoid, naiveSoftmaxLossAndGradient, negSamplingLossAndGradient, skipgram$  å››ä¸ªå‡½æ•°ï¼Œä¸»è¦è€ƒå¯Ÿå¯¹ç¬¬ä¸€éƒ¨åˆ†ä¸­åå‘ä¼ æ’­è®¡ç®—ç»“æœçš„å®ç°ã€‚ä»£ç å®ç°ä¸­ï¼Œé€šè¿‡ä¼˜åŒ–åå¯¼æ•°ç»“åˆåå¯¼æ•°è®¡ç®—ç»“æœä¸ $\sigma(x) + \sigma(-x) = 1$ å¯¹å…¬å¼è¿›è¡Œè½¬åŒ–ï¼Œä»è€Œå®ç°äº†å…¨çŸ¢é‡åŒ–ã€‚è¿™éƒ¨åˆ†éœ€è¦å¤§å®¶è‡ªè¡Œç»“åˆä»£ç ä¸å…¬å¼è¿›è¡Œæ¨å¯¼ã€‚

#### sgd.py

å®ç° SGD 
$$
\theta^{n e w}=\theta^{o l d} - \alpha \nabla_{\theta} J(\theta)
$$

#### run.py

é¦–å…ˆè¦è¯´æ˜çš„æ˜¯ï¼Œè¿™ä¸ªçœŸçš„è¦è·‘å¥½ä¹… ğŸ˜…

!!! question "Question"

```
Briefly explain in at most three sentences what you see in the plot.
```

ä¸Šå›¾æ˜¯ç»è¿‡è®­ç»ƒçš„è¯å‘é‡çš„å¯è§†åŒ–ã€‚æˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°ä¸€äº›æ¨¡å¼ï¼š

-   è¿‘ä¹‰è¯è¢«ç»„åˆåœ¨ä¸€èµ·ï¼Œæ¯”å¦‚ amazing å’Œ wonderfulï¼Œwoman å’Œ femaleã€‚
    -   ä½†æ˜¯ man å’Œ male å´è·ç¦»è¾ƒè¿œ
-   åä¹‰è¯å¯èƒ½å› ä¸ºç»å¸¸å±äºåŒä¸€ä¸Šä¸‹æ–‡ï¼Œå®ƒä»¬ä¹Ÿä¼šä¸åŒä¹‰è¯ä¸€èµ·å‡ºç°ï¼Œæ¯”å¦‚ enjoyable å’Œ annoyingã€‚
-   `man:king::woman:queen` ä»¥åŠ `queen:king::female:male` å½¢æˆçš„ä¸¤æ¡ç›´çº¿åŸºæœ¬å¹³è¡Œ

## Reference

-   [ä»SVDåˆ°PCAâ€”â€”å¥‡å¦™çš„æ•°å­¦æ¸¸æˆ](

-   <https://my.oschina.net/findbill/blog/535044>)